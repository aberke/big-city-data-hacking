{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city philly\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Processes the raw bike trip data to get information on bike dock locations\n",
    "and when the docks were placed in those locations.\n",
    "\n",
    "Desired output columns:\n",
    "\n",
    "id | first | last | name | lat | lon | rides\n",
    "\n",
    "where\n",
    "- id is the station's id\n",
    "- first is the earliest trip date for the station id\n",
    "- last is the latest trip date for the station id (included in case docks are removed)\n",
    "- name is the station's name\n",
    "- lat and lon are the latitude and longitude of the station's location\n",
    "- rides is a count of the number of rides found in the data -- it is used to remove dummy stations in the data.\n",
    "    only stations with more than RIDES_COUNT_THRESHOLD are included in output\n",
    "\n",
    "This script is abstracte to apply to multiple cities.\n",
    "DON'T FORGET: update the 'CITY' variable\n",
    "\n",
    "\"\"\"\n",
    "from datetime import datetime\n",
    "import math\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "# CITY = 'dc'\n",
    "# CITY = 'boston'\n",
    "# CITY = 'nyc'\n",
    "# CITY = 'chicago'\n",
    "# CITY = 'la'\n",
    "CITY = 'philly'\n",
    "print('city', CITY)\n",
    "\n",
    "\n",
    "RIDES_COUNT_THRESHOLD = 100\n",
    "\n",
    "\n",
    "def get_filepath(city):\n",
    "    return '../data/' + city + '-bike/'\n",
    "\n",
    "\n",
    "def transform_date(date):\n",
    "    try:\n",
    "        dt = datetime.strptime(date.split(' ')[0], '%m/%d/%Y')\n",
    "    except ValueError:\n",
    "        # this dataset is so frustrating lol\n",
    "        dt = datetime.strptime(date.split(' ')[0], '%Y-%m-%d')\n",
    "        \n",
    "    return dt.strftime('%Y-%m-%d')\n",
    "\n",
    "def open_zipfile(zipfilename):\n",
    "    # Because someone dropped some gnarly mac osx files into their zips\n",
    "    zipfile = ZipFile(zipfilename)\n",
    "    filenames = [f.filename for f in zipfile.infolist()]\n",
    "    # Return the first file that can be opened  - not all of them have .csv suffix\n",
    "    for filename in filenames:\n",
    "        try:\n",
    "            df = pd.read_csv(zipfile.open(filename))\n",
    "            return df\n",
    "        except:\n",
    "            print('failed to open filename from zip', zipfilename, ': ', filename)\n",
    "            pass\n",
    "    raise Exception('unable to read a csv from zipfile %s' % zipfilename)\n",
    "\n",
    "def open_zipfile_dc(zipfilename):\n",
    "## DC bike files from 2012 to 2017 have 4 files for each quarter\n",
    "## this generator yields each of those files\n",
    "\n",
    "    zipfile = ZipFile(zipfilename)\n",
    "    files = [f.filename for f in zipfile.infolist()]\n",
    "\n",
    "    for filename in files:\n",
    "        try:\n",
    "            df = pd.read_csv(zipfile.open(filename))\n",
    "            yield df\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "def open_zipfile_chicago(zipfilename):\n",
    "\n",
    "    zipfile = ZipFile(zipfilename)\n",
    "    files = [f.filename for f in zipfile.infolist()]\n",
    "    for filename in files:\n",
    "        if filename[:8] == '__MACOSX' or filename.endswith('txt') or 'Divvy_Stations_2' in filename:\n",
    "            continue\n",
    "        if '/' in filename:\n",
    "            if '/Divvy_Stations' in filename:\n",
    "                continue\n",
    "        try:\n",
    "            print(filename)\n",
    "            df = pd.read_csv(zipfile.open(filename))\n",
    "            yield df\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "def open_zipfile_la(zipfilename):\n",
    "    zipfile = ZipFile(zipfilename)\n",
    "    filenames = [f.filename for f in zipfile.infolist()]\n",
    "    # Return the first file that can be opened  - not all of them have .csv suffix\n",
    "    if len(filenames) == 2:\n",
    "        filename = filenames[1]\n",
    "    else:\n",
    "        filename = filenames[0]\n",
    "\n",
    "\n",
    "   \n",
    "    try:\n",
    "        df = pd.read_csv(zipfile.open(filename))\n",
    "        return df\n",
    "    except:\n",
    "        print('failed to open filename from zip', zipfilename, ': ', filename)\n",
    "        pass\n",
    "    raise Exception('unable to read a csv from zipfile %s' % zipfilename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "make a dict like \n",
    "{\"id\": {\"name\": \"\", \"lat\": \"\", \"lon\": \"\", \"first\": \"\", \"last\": \"\"}}\n",
    "where there is one entry for each id\n",
    "and where the start time is always the earliest found\n",
    "\n",
    "and then later transform it into a dict like\n",
    "\n",
    "{'id': [id1, id2, id3], 'col_2': ['a', 'b', 'c', 'd']}\n",
    "\n",
    "to then make into a dataframe and save as a CSV\n",
    "\"\"\"\n",
    "\n",
    "# input file column names for indexing data with\n",
    "start_station_id = 'startstationid'\n",
    "start_station_name = 'startstationname'\n",
    "start_station_latitude = 'startstationlatitude'\n",
    "start_station_longitude = 'startstationlongitude'\n",
    "starttime = 'starttime'\n",
    "\n",
    "\n",
    "    \n",
    "# output file column names\n",
    "ID = 'id'\n",
    "NAME = 'name'\n",
    "LAT = 'lat'\n",
    "LON = 'lon'\n",
    "FIRST = 'first'\n",
    "LAST = 'last'\n",
    "RIDES = 'rides'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_stations_df(df):\n",
    "    # Because someone can't make data files with uniform column names\n",
    "    df.columns = df.columns.str.replace('number', 'id')  # 'Station Number' vs Station ID\n",
    "    df.columns = df.columns.str.replace('date', 'time')  # 'Start Date' vs 'Start Time'\n",
    "    \n",
    "    \n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    df.columns = df.columns.str.replace('[\\ ]', '')\n",
    "    \n",
    "    # transform the dates\n",
    "    df[starttime] = df[starttime].apply(transform_date)\n",
    "    if CITY == \"boston\":\n",
    "        df = preprocess_boston_stations_df(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Some of the earlier bostons stations data does not include lat,lon coordinates.\n",
    "# These files contains the lat,lon coordinates (and other data) for station IDs\n",
    "hubway_stations_locations_filenames = [\n",
    "    \"Hubway_Stations_as_of_July_2017.csv\",\n",
    "    \"previous_Hubway_Stations_as_of_July_2017.csv\"\n",
    "]\n",
    "\n",
    "def get_hubway_stations_locations_df():\n",
    "    df = pd.DataFrame()\n",
    "    filenames = [get_filepath(CITY) + fname for fname in hubway_stations_locations_filenames]\n",
    "    for filename in filenames:\n",
    "        new_df = pd.read_csv(filename)    \n",
    "        hubway_stations_locations_column_names = {\n",
    "            \"Station ID\": start_station_id,\n",
    "            \"Latitude\": start_station_latitude,\n",
    "            \"Longitude\": start_station_longitude,\n",
    "        }\n",
    "        # Rename the column names to match the rides data that the locations data will be joined with\n",
    "        new_df.rename(columns=hubway_stations_locations_column_names, inplace=True)\n",
    "        df = new_df if df.empty else df.append(new_df)\n",
    "    df.drop_duplicates(subset=[start_station_id], inplace=True)\n",
    "    return df\n",
    "\n",
    "hubway_stations_locations_df = None\n",
    "if CITY == \"boston\":\n",
    "    hubway_stations_locations_df = get_hubway_stations_locations_df()\n",
    "\n",
    "\n",
    "def preprocess_boston_stations_df(df):\n",
    "    if start_station_latitude in df.columns:\n",
    "        return df\n",
    "    # Otherwise this is one of the datasets that is lacking lat, lon info.\n",
    "    # Add the lat,lon info\n",
    "    return hubway_stations_locations_df.merge(df, on=start_station_id)\n",
    "\n",
    "def choose_chicago_columns(filename):\n",
    "    if filename == 'Divvy_Trips_2018_Q1.zip' or filename == 'Divvy_Trips_2019_Q2.zip':\n",
    "        return ('03-rentalstartstationid','03-rentalstartstationname', '', '', '01-rentaldetailslocalstarttime' )\n",
    "    if filename[:5] == 'Divvy' and filename != 'Divvy_Trips_2020_Q1.zip':\n",
    "        if filename[:7] == 'Divvy_S' or int(filename[12:16]) < 2017:\n",
    "            return ('from_station_id', 'from_station_name', '', '', 'starttime')\n",
    "        return ('from_station_id', 'from_station_name', '', '', 'start_time')\n",
    "    return ('start_station_id', 'start_station_name', 'start_lat', 'start_lng', 'started_at')\n",
    "\n",
    "\n",
    "\n",
    "# hubway_stations_locations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indego-stations-2022-04-01.csv\n",
      "indego-trips-2017-q3.csv.zip\n",
      "0 : handling file indego-trips-2017-q3.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indego-trips-2017-q4.csv.zip\n",
      "1 : handling file indego-trips-2017-q4.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indego-trips-2018-q1.csv.zip\n",
      "2 : handling file indego-trips-2018-q1.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indego-trips-2018-q2.csv.zip\n",
      "3 : handling file indego-trips-2018-q2.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indego-trips-2018-q3.csv.zip\n",
      "4 : handling file indego-trips-2018-q3.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indego-trips-2018-q4.csv.zip\n",
      "5 : handling file indego-trips-2018-q4.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\2226592492.py:110: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(zipfile.open(filename))\n",
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indego-trips-2019-q1.csv.zip\n",
      "6 : handling file indego-trips-2019-q1.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indego-trips-2019-q2.csv.zip\n",
      "7 : handling file indego-trips-2019-q2.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indego-trips-2019-q3-1.zip\n",
      "8 : handling file indego-trips-2019-q3-1.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indego-trips-2019-q4.csv.zip\n",
      "9 : handling file indego-trips-2019-q4.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indego-trips-2020-q1.csv.zip\n",
      "10 : handling file indego-trips-2020-q1.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indego-trips-2020-q2.zip\n",
      "11 : handling file indego-trips-2020-q2.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indego-trips-2020-q3.zip\n",
      "12 : handling file indego-trips-2020-q3.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\2226592492.py:110: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(zipfile.open(filename))\n",
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indego-trips-2020-q4.zip\n",
      "13 : handling file indego-trips-2020-q4.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indego-trips-2021-q1.zip\n",
      "14 : handling file indego-trips-2021-q1.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indego-trips-2021-q2.zip\n",
      "15 : handling file indego-trips-2021-q2.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\2226592492.py:110: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(zipfile.open(filename))\n",
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indego-trips-2021-q3.zip\n",
      "16 : handling file indego-trips-2021-q3.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\2226592492.py:110: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(zipfile.open(filename))\n",
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indego-trips-2021-q4.zip\n",
      "17 : handling file indego-trips-2021-q4.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\2226592492.py:110: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(zipfile.open(filename))\n",
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indego-trips-2022-q1.zip\n",
      "18 : handling file indego-trips-2022-q1.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indego_gbfs_trips_Q1_2017.zip\n",
      "19 : handling file indego_gbfs_trips_Q1_2017.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indego_gbfs_trips_Q2_2017.csv.zip\n",
      "20 : handling file indego_gbfs_trips_Q2_2017.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indego_Trips_2016Q1.zip\n",
      "21 : handling file Indego_Trips_2016Q1.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indego_Trips_2016Q2.zip\n",
      "22 : handling file Indego_Trips_2016Q2.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indego_trips_Q4_2016.zip\n",
      "23 : handling file Indego_trips_Q4_2016.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3_2016_trips.zip\n",
      "24 : handling file Q3_2016_trips.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter\\AppData\\Local\\Temp\\ipykernel_16592\\4204538325.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[\\ ]', '')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>rides</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3160</td>\n",
       "      <td>38th &amp; Market</td>\n",
       "      <td>39.956619</td>\n",
       "      <td>-75.198624</td>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>26285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3046</td>\n",
       "      <td>2nd &amp; Market</td>\n",
       "      <td>39.950119</td>\n",
       "      <td>-75.144722</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>58447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3006</td>\n",
       "      <td>40th &amp; Spruce</td>\n",
       "      <td>39.952202</td>\n",
       "      <td>-75.203110</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>40390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3010</td>\n",
       "      <td>15th &amp; Spruce</td>\n",
       "      <td>39.947109</td>\n",
       "      <td>-75.166183</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>109658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3070</td>\n",
       "      <td>Spring Garden Station, MFL</td>\n",
       "      <td>39.960621</td>\n",
       "      <td>-75.139832</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>20058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                        name        lat        lon       first  \\\n",
       "0  3160               38th & Market  39.956619 -75.198624  2017-04-01   \n",
       "1  3046                2nd & Market  39.950119 -75.144722  2016-01-01   \n",
       "2  3006               40th & Spruce  39.952202 -75.203110  2016-01-01   \n",
       "3  3010               15th & Spruce  39.947109 -75.166183  2016-01-01   \n",
       "4  3070  Spring Garden Station, MFL  39.960621 -75.139832  2016-01-01   \n",
       "\n",
       "         last   rides  \n",
       "0  2022-03-31   26285  \n",
       "1  2022-03-31   58447  \n",
       "2  2022-03-31   40390  \n",
       "3  2022-03-31  109658  \n",
       "4  2022-03-31   20058  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILENAMES_TO_IGNORE = hubway_stations_locations_filenames + ['indego-stations-2022-04-01.csv'] + ['stations.csv'] + ['stations.json'] + ['metro-bike-share-stations-2022-04-01.csv']# + [more bad filenames here]\n",
    "\n",
    "\n",
    "def stations_dict_to_df(stations_dict):\n",
    "    new_dict = {\n",
    "        ID: [],\n",
    "        NAME: [],\n",
    "        LAT: [],\n",
    "        LON: [],\n",
    "        FIRST: [],\n",
    "        LAST: [],\n",
    "        RIDES: []\n",
    "    }\n",
    "    for station_id, station_dict in stations_dict.items():\n",
    "        new_dict[ID].append(station_id)\n",
    "        new_dict[NAME].append(station_dict[NAME])\n",
    "        new_dict[LAT].append(station_dict[LAT])\n",
    "        new_dict[LON].append(station_dict[LON])\n",
    "        new_dict[FIRST].append(station_dict[FIRST])\n",
    "        new_dict[LAST].append(station_dict[LAST])\n",
    "        new_dict[RIDES].append(station_dict[RIDES])\n",
    "    \n",
    "    return pd.DataFrame.from_dict(new_dict)\n",
    "    \n",
    "\n",
    "\n",
    "stations_dict = dict()\n",
    "needs_lat_lon = set()\n",
    "directory = get_filepath(CITY)\n",
    "files_count = 0\n",
    "for filename in os.listdir(directory):\n",
    "    \n",
    "    print(filename)\n",
    "    if filename in FILENAMES_TO_IGNORE:# or filename[:6] not in ['202102']:\n",
    "        continue\n",
    "    if CITY == 'chicago':\n",
    "        start_station_id, start_station_name, start_station_latitude, start_station_longitude, starttime = choose_chicago_columns(filename)\n",
    "    elif CITY == 'la' or CITY == 'philly':\n",
    "        \n",
    "        if (filename.endswith(\"-2.zip\") and CITY == 'la') or ('2016' in filename or 'Q1_2017' in filename and CITY == 'philly'):\n",
    "            start_station_id = 'start_station_id'\n",
    "            start_station_name = 'start_station_id'\n",
    "        else:\n",
    "            start_station_id = 'start_station'\n",
    "            start_station_name = 'start_station' # to fill in with other data table later\n",
    "        start_station_latitude = 'start_lat'\n",
    "        start_station_longitude = 'start_lon'\n",
    "        starttime = 'start_time'\n",
    "   \n",
    "\n",
    "    elif filename[:4] in ['2021', '2022'] and filename[:6] not in ['202101'] and CITY == 'boston' or (filename[:4] in ['2020', '2021', '2022'] and filename[:6] not in ['202001', '202002', '202003']) and CITY == 'dc' :\n",
    "        start_station_id = 'start_station_id'\n",
    "        start_station_name = 'start_station_name'\n",
    "        start_station_latitude = 'start_lat'\n",
    "        start_station_longitude = 'start_lng'\n",
    "        starttime = 'started_at'\n",
    "    elif CITY != 'dc':\n",
    "        start_station_id = 'startstationid'\n",
    "        start_station_name = 'startstationname'\n",
    "        start_station_latitude = 'startstationlatitude'\n",
    "        start_station_longitude = 'startstationlongitude'\n",
    "        starttime = 'starttime'\n",
    "    else:\n",
    "        start_station_id = 'startstationid'\n",
    "        start_station_name = 'startstation'\n",
    "        start_station_latitude = \"\"\n",
    "        start_station_longitude = \"\"\n",
    "        starttime = \"starttime\"\n",
    "\n",
    "\n",
    "    fullfilename = directory + filename\n",
    "    print(files_count, ': handling file', filename)\n",
    "    files_count+=1\n",
    "    \n",
    "    if filename.endswith(\".csv\"):\n",
    "        stations_dfs = [pd.read_csv(fullfilename)]\n",
    "    elif filename.endswith(\".zip\") and CITY == 'ny':\n",
    "        stations_dfs = [open_zipfile(fullfilename)]\n",
    "    elif filename.endswith(\".zip\") and CITY == 'la' or CITY =='philly':\n",
    "        stations_dfs = [open_zipfile_la(fullfilename)]\n",
    "    elif filename.endswith(\".zip\") and CITY == 'dc':\n",
    "        stations_dfs = [df for df in open_zipfile_dc(fullfilename)]\n",
    "    elif filename.endswith(\".zip\") and CITY == 'chicago':\n",
    "        stations_dfs = [df for df in open_zipfile_chicago(fullfilename)]\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    for stations_df in stations_dfs:\n",
    "\n",
    "        stations_df = preprocess_stations_df(stations_df)\n",
    "        \n",
    "        unique_station_ids = stations_df[start_station_id].unique()\n",
    "        for station_id in unique_station_ids:\n",
    "            station_df = stations_df[stations_df[start_station_id] == station_id]\n",
    "            \n",
    "            if station_id not in stations_dict:\n",
    "                try:\n",
    "                    stations_dict[station_id] = {\n",
    "                        NAME: station_df[start_station_name].iloc[0], \n",
    "                        LAT: station_df[start_station_latitude].iloc[0],\n",
    "                        LON: station_df[start_station_longitude].iloc[0], \n",
    "                        FIRST: station_df[starttime].iloc[0], \n",
    "                        LAST: station_df[starttime].iloc[0],\n",
    "                        RIDES: 0,\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    \n",
    "                    if type(e).__name__ == 'KeyError':\n",
    "                        \n",
    "                        stations_dict[station_id] = {\n",
    "                            NAME: station_df[start_station_name].iloc[0], \n",
    "                            LAT: 0,\n",
    "                            LON: 0,\n",
    "                            FIRST: station_df[starttime].iloc[0], \n",
    "                            LAST: station_df[starttime].iloc[0],\n",
    "                            RIDES: 0,\n",
    "                        }\n",
    "                        needs_lat_lon.add(station_id)\n",
    "                    else:    \n",
    "                        continue\n",
    "            if station_id in needs_lat_lon and starttime == 'started_at':\n",
    "                stations_dict[station_id][LAT] = station_df[start_station_latitude].iloc[0]\n",
    "                stations_dict[station_id][LON] = station_df[start_station_longitude].iloc[0]\n",
    "                needs_lat_lon.remove(station_id)\n",
    "            rides_count = len(station_df.index)\n",
    "            stations_dict[station_id][RIDES] += rides_count\n",
    "            station_df = station_df.sort_values(by=[starttime])\n",
    "            if (station_df[starttime].iloc[0] < stations_dict[station_id][FIRST]):\n",
    "                stations_dict[station_id][FIRST] = stations_df[starttime].iloc[0]\n",
    "            if (station_df[starttime].iloc[-1] > stations_dict[station_id][LAST]):\n",
    "                stations_dict[station_id][LAST] = stations_df[starttime].iloc[-1]\n",
    "\n",
    "if CITY == 'la':\n",
    "    '''Adding station names to the data frame for LA'''\n",
    "    name_df = pd.read_csv(get_filepath(CITY) + '/metro-bike-share-stations-2022-04-01.csv', encoding='cp1252')\n",
    "    for index, row in name_df.iterrows():\n",
    "        if row['Station_ID'] in stations_dict:\n",
    "            stations_dict[row['Station_ID']][NAME] = row['Station_Name']\n",
    "if CITY == 'philly':\n",
    "    name_df = pd.read_csv(get_filepath(CITY) + '/indego-stations-2022-04-01.csv')\n",
    "    for index, row in name_df.iterrows():\n",
    "        if row['Station_ID'] in stations_dict:\n",
    "            stations_dict[row['Station_ID']][NAME] = row['Station_Name']\n",
    "\n",
    "\n",
    "stations_df = stations_dict_to_df(stations_dict)\n",
    "stations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing 8 bad stations that each have less than 100 rides from stations data\n",
      "removing 0 more stations for not having latitude or longitude\n"
     ]
    }
   ],
   "source": [
    "# Transform the stations_df\n",
    "\n",
    "# Remove dummy stations (there are test stations in the data)\n",
    "# Remove stations with less than RIDES_COUNT_THRESHOLD rides\n",
    "bad_stations_df = stations_df[stations_df[RIDES] < RIDES_COUNT_THRESHOLD]\n",
    "print('removing %d bad stations that each have less than %d rides from stations data' % (bad_stations_df.shape[0], RIDES_COUNT_THRESHOLD))\n",
    "stations_df = stations_df[stations_df[RIDES] >= RIDES_COUNT_THRESHOLD]\n",
    "## Remove stations that do not have a latitude and longitude measure\n",
    "bad_stations_2 = stations_df[stations_df[LAT] == 0]\n",
    "print(f'removing {bad_stations_2.shape[0]} more stations for not having latitude or longitude')\n",
    "stations_df = stations_df[stations_df[LAT] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>rides</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>3083</td>\n",
       "      <td>3083</td>\n",
       "      <td>39.972328</td>\n",
       "      <td>-75.145020</td>\n",
       "      <td>2017-09-12</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>90007</td>\n",
       "      <td>90007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>2018-06-30</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>90018</td>\n",
       "      <td>90018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-02-21</td>\n",
       "      <td>2018-06-30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>90010</td>\n",
       "      <td>90010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-06-19</td>\n",
       "      <td>2018-06-19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>3042</td>\n",
       "      <td>3042</td>\n",
       "      <td>39.949421</td>\n",
       "      <td>-75.166130</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>3271</td>\n",
       "      <td>53rd &amp; Baltimore</td>\n",
       "      <td>39.947601</td>\n",
       "      <td>-75.229462</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>3272</td>\n",
       "      <td>50th &amp; Haverford</td>\n",
       "      <td>39.964050</td>\n",
       "      <td>-75.220482</td>\n",
       "      <td>2022-03-11</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>3265</td>\n",
       "      <td>Powelton &amp; Lancaster</td>\n",
       "      <td>39.959572</td>\n",
       "      <td>-75.196327</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                  name        lat        lon       first  \\\n",
       "119   3083                  3083  39.972328 -75.145020  2017-09-12   \n",
       "127  90007                 90007        NaN        NaN  2018-02-02   \n",
       "128  90018                 90018        NaN        NaN  2018-02-21   \n",
       "130  90010                 90010        NaN        NaN  2018-06-19   \n",
       "150   3042                  3042  39.949421 -75.166130  2019-10-01   \n",
       "190   3271      53rd & Baltimore  39.947601 -75.229462  2022-03-03   \n",
       "191   3272      50th & Haverford  39.964050 -75.220482  2022-03-11   \n",
       "192   3265  Powelton & Lancaster  39.959572 -75.196327  2022-03-31   \n",
       "\n",
       "           last  rides  \n",
       "119  2017-12-31     10  \n",
       "127  2018-06-30     15  \n",
       "128  2018-06-30      2  \n",
       "130  2018-06-19      1  \n",
       "150  2019-12-31     21  \n",
       "190  2022-03-31     40  \n",
       "191  2022-03-31     34  \n",
       "192  2022-03-31      2  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_stations_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"For the boston hubway/blue bikes data there will be duplicates because\n",
    "when management changed from hubway to Bluebikes, the data fromat did too\n",
    "This includes the station id/numbers and names AND lat/lon!\n",
    "Task: deduplicate stations\n",
    "\n",
    "Idea to understand data: sort the stations so the potential duplicates are next to each other\n",
    "when merging/deduping data make sure to keep the earliest first and the latest last.\n",
    "\n",
    "approach to deduplicating stations:\n",
    "- normalize names and add new temporary column with normalized name\n",
    "- get list of unique normalized names\n",
    "- for each name:\n",
    "    make a df for that name, sorted by [first, last]\n",
    "    update main df to replace entries with that name with:\n",
    "        first first\n",
    "        last last\n",
    "        last name\n",
    "        rides as sum of rides\n",
    "    sort main df by [name, first] and drop duplicates (duplicates on normalized name)\n",
    "    remove normalized name column\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "NORMALIZED_NAME = 'normalized_name'\n",
    "\n",
    "def normalized_station_name(name):\n",
    "    normalized_name = name.lower()\n",
    "    normalized_name = normalized_name.replace(\"former\",  \"\").replace(\" \", \"\")\n",
    "    normalized_name = re.sub(r'[^a-z0-9]','', normalized_name)\n",
    "    return normalized_name\n",
    "\n",
    "if CITY == 'boston' or CITY == 'chicago':\n",
    "    stations_df[NORMALIZED_NAME] = stations_df[NAME].apply(normalized_station_name)\n",
    "    normalized_names = stations_df[NORMALIZED_NAME]\n",
    "    print(normalized_names.shape[0], ' names')\n",
    "    unique_normalized_names = stations_df[NORMALIZED_NAME].unique()\n",
    "    print(unique_normalized_names.shape[0], ' unique normalized names') #, unique_normalized_names)\n",
    "\n",
    "\n",
    "    n = 0\n",
    "    for normalized_name in unique_normalized_names:\n",
    "        print(n, 'handling name', normalized_name)\n",
    "        n+=1\n",
    "        name_df = stations_df[stations_df[NORMALIZED_NAME] == normalized_name]\n",
    "        name_df.sort_values(by=[FIRST, LAST], inplace=True)\n",
    "        first = name_df[FIRST].iloc[0]\n",
    "        last = name_df[LAST].iloc[-1]\n",
    "        name = name_df[NAME].iloc[-1]\n",
    "        rides = name_df[RIDES].sum()\n",
    "        update_condition = (stations_df[NORMALIZED_NAME] == normalized_name)\n",
    "        stations_df.loc[update_condition, [FIRST, LAST, NAME, RIDES]] = first, last, name, rides\n",
    "\n",
    "    stations_dropped_duplicates_df = stations_df.drop_duplicates(subset=[NORMALIZED_NAME])\n",
    "    print('dropped %s rows based on duplicate names' % (int(stations_df.shape[0]) - int(stations_dropped_duplicates_df.shape[0])))\n",
    "    stations_dropped_duplicates_df.drop(labels=[NORMALIZED_NAME], axis=1, inplace=True)\n",
    "    \n",
    "    stations_df = stations_dropped_duplicates_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote data to  ../data/philly-bike/stations.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the data to CSV\n",
    "save_to_csvfilename = directory + 'stations.csv'\n",
    "stations_df.to_csv(save_to_csvfilename)\n",
    "print('wrote data to ', save_to_csvfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to stations.json\n"
     ]
    }
   ],
   "source": [
    "# Save the data to JSON that will be used in web app\n",
    "import json\n",
    "\n",
    "stations = []\n",
    "for index, row in stations_df.iterrows():\n",
    "    # Transform the date\n",
    "    date = row[5]\n",
    "    \n",
    "    stations.append({\n",
    "        ID: str(row[ID]),\n",
    "        NAME: str(row[NAME]),\n",
    "        LAT: row[LAT],\n",
    "        LON: row[LON],\n",
    "        FIRST: transform_date(row[FIRST]),\n",
    "        LAST: transform_date(row[LAST]),\n",
    "    })\n",
    "\n",
    "json = json.dumps(stations)\n",
    "\n",
    "save_to_jsonfilename = directory + 'stations.json'\n",
    "with open(save_to_jsonfilename, 'w') as f:\n",
    "    f.write(json)\n",
    "print(\"Data written to stations.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>rides</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3030</td>\n",
       "      <td>Main &amp; 1st</td>\n",
       "      <td>34.051941</td>\n",
       "      <td>-118.243530</td>\n",
       "      <td>2016-07-07</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>38267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3028</td>\n",
       "      <td>Grand &amp; Temple</td>\n",
       "      <td>34.058319</td>\n",
       "      <td>-118.246094</td>\n",
       "      <td>2016-07-07</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>7256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3027</td>\n",
       "      <td>Spring &amp; 3rd</td>\n",
       "      <td>34.049980</td>\n",
       "      <td>-118.247162</td>\n",
       "      <td>2016-07-07</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>15970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3007</td>\n",
       "      <td>5th &amp; Grand</td>\n",
       "      <td>34.050480</td>\n",
       "      <td>-118.254593</td>\n",
       "      <td>2016-07-07</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>20320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3066</td>\n",
       "      <td>Spring &amp; College</td>\n",
       "      <td>34.063389</td>\n",
       "      <td>-118.236160</td>\n",
       "      <td>2016-07-07</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>12352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id              name        lat         lon       first        last  \\\n",
       "0  3030        Main & 1st  34.051941 -118.243530  2016-07-07  2022-03-31   \n",
       "1  3028    Grand & Temple  34.058319 -118.246094  2016-07-07  2022-03-31   \n",
       "2  3027      Spring & 3rd  34.049980 -118.247162  2016-07-07  2022-03-31   \n",
       "3  3007       5th & Grand  34.050480 -118.254593  2016-07-07  2022-03-31   \n",
       "4  3066  Spring & College  34.063389 -118.236160  2016-07-07  2022-03-31   \n",
       "\n",
       "   rides  \n",
       "0  38267  \n",
       "1   7256  \n",
       "2  15970  \n",
       "3  20320  \n",
       "4  12352  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "639955f742b38ffeb680f958c1dc9e296c7318d3e6d17e3c3dcc92b8320cf293"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('geo_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
